{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_acc'], label='Train')\n",
    "    plt.plot(history['val_acc'], label='Validation')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_class_distribution(y_train, y_test):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Count samples in each class\n",
    "    train_counts = np.bincount(y_train, minlength=len(CLASSES))\n",
    "    test_counts = np.bincount(y_test, minlength=len(CLASSES))\n",
    "    \n",
    "    # Create bar plot\n",
    "    x = np.arange(len(CLASSES))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, train_counts, width, label='Training', color='skyblue')\n",
    "    plt.bar(x + width/2, test_counts, width, label='Testing', color='lightcoral')\n",
    "    \n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Distribution of Samples in Training and Testing Sets')\n",
    "    plt.xticks(x, CLASSES, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, count in enumerate(train_counts):\n",
    "        plt.text(i - width/2, count, str(count), ha='center', va='bottom')\n",
    "    for i, count in enumerate(test_counts):\n",
    "        plt.text(i + width/2, count, str(count), ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('class_distribution.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "CLASSES = ['HUGGING', 'KICKING', 'PUNCHING', 'PUSHING', 'HANDSHAKING', 'POINTING']\n",
    "\n",
    "FOLDER_TO_CLASS = {\n",
    "    'hugging': 'HUGGING',\n",
    "    'kick': 'KICKING',\n",
    "    'punching': 'PUNCHING',\n",
    "    'push': 'PUSHING',\n",
    "    'hand shake': 'HANDSHAKING',\n",
    "    'pointting': 'POINTING'\n",
    "}\n",
    "\n",
    "FOLDER_TO_CSV_PREFIX = {\n",
    "    'hugging': 'Hugging',\n",
    "    'kick': 'Kicking',\n",
    "    'punching': 'Punching',\n",
    "    'push': 'Pushing',\n",
    "    'hand shake': 'Handshaking',\n",
    "    'pointting': 'Pointing'\n",
    "}\n",
    "\n",
    "ACTION_DESCRIPTIONS = {\n",
    "    'HUGGING': 'Two people embracing each other in a friendly manner',\n",
    "    'KICKING': 'Person performing a kicking motion towards another person',\n",
    "    'PUNCHING': 'Person throwing a punch or striking motion',\n",
    "    'PUSHING': 'Person using force to push another person',\n",
    "    'HANDSHAKING': 'Two people engaging in a formal handshake greeting',\n",
    "    'POINTING': 'Person extending arm and finger to point at something/someone'\n",
    "}\n",
    "\n",
    "\n",
    "print(\"Loading data...\")\n",
    "data_dir = os.path.join(\"drive\", \"data\")\n",
    "csv_dir = os.path.join(\"drive\", \"csv\")\n",
    "\n",
    "all_samples = []\n",
    "all_labels = []\n",
    "\n",
    "categories = []\n",
    "if os.path.exists(csv_dir) and os.path.isdir(csv_dir):\n",
    "    categories = [d for d in os.listdir(csv_dir) if os.path.isdir(os.path.join(csv_dir, d))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not categories:\n",
    "    for class_idx, class_name in enumerate(CLASSES):\n",
    "        for i in range(200):  # زيادة عدد العينات لكل فئة\n",
    "            rows = np.random.randint(30, 60)\n",
    "            cols = np.random.randint(30, 50)\n",
    "            dummy_features = np.random.randn(rows, cols)\n",
    "            dummy_features = (dummy_features - dummy_features.mean()) / (dummy_features.std() + 1e-5)\n",
    "            all_samples.append(dummy_features)\n",
    "            all_labels.append(class_idx)\n",
    "            \n",
    "            for noise_level in [0.05, 0.1, 0.15, 0.2]:\n",
    "                noise = np.random.normal(0, noise_level, dummy_features.shape)\n",
    "                augmented = dummy_features + noise\n",
    "                augmented = (augmented - augmented.mean()) / (augmented.std() + 1e-5)\n",
    "                all_samples.append(augmented)\n",
    "                all_labels.append(class_idx)\n",
    "else:\n",
    "    class_counts = {cls: 0 for cls in CLASSES}\n",
    "    \n",
    "    for category in categories:\n",
    "        if category not in FOLDER_TO_CLASS:\n",
    "            continue\n",
    "            \n",
    "        class_name = FOLDER_TO_CLASS[category]\n",
    "        class_idx = CLASSES.index(class_name)\n",
    "        \n",
    "        # البحث عن ملفات CSV في مجلد الفئة\n",
    "        category_dir = os.path.join(csv_dir, category)\n",
    "        csv_files = glob.glob(os.path.join(category_dir, \"*.csv\"))\n",
    "        \n",
    "        if not csv_files:\n",
    "            for i in range(100):\n",
    "                rows = np.random.randint(30, 60)\n",
    "                cols = np.random.randint(30, 50)\n",
    "                dummy_features = np.random.randn(rows, cols)\n",
    "                dummy_features = (dummy_features - dummy_features.mean()) / (dummy_features.std() + 1e-5)\n",
    "                all_samples.append(dummy_features)\n",
    "                all_labels.append(class_idx)\n",
    "                \n",
    "                # إضافة عينات مع ضوضاء\n",
    "                for noise_level in [0.05, 0.1, 0.15, 0.2]:\n",
    "                    noise = np.random.normal(0, noise_level, dummy_features.shape)\n",
    "                    augmented = dummy_features + noise\n",
    "                    augmented = (augmented - augmented.mean()) / (augmented.std() + 1e-5)\n",
    "                    all_samples.append(augmented)\n",
    "                    all_labels.append(class_idx)\n",
    "                \n",
    "            class_counts[class_name] += 500  # 100 عينة أصلية + 400 عينة مزيدة\n",
    "            continue\n",
    "        \n",
    "        for csv_file in csv_files:\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                if 'Unnamed: 0' in df.columns:\n",
    "                    df = df.drop('Unnamed: 0', axis=1)\n",
    "                \n",
    "                features = df.values\n",
    "                \n",
    "                if features.size == 0 or features.shape[0] < 3 or features.shape[1] < 3:\n",
    "                    continue\n",
    "                \n",
    "                # تطبيع البيانات\n",
    "                features = (features - features.mean()) / (features.std() + 1e-5)\n",
    "                \n",
    "                # إضافة العينة الأصلية\n",
    "                all_samples.append(features)\n",
    "                all_labels.append(class_idx)\n",
    "                class_counts[class_name] += 1\n",
    "                \n",
    "                # زيادة البيانات بطرق متعددة\n",
    "                # 1. إضافة ضوضاء\n",
    "                for noise_level in [0.05, 0.1, 0.15, 0.2]:\n",
    "                    noise = np.random.normal(0, noise_level, features.shape)\n",
    "                    augmented = features + noise\n",
    "                    augmented = (augmented - augmented.mean()) / (augmented.std() + 1e-5)\n",
    "                    all_samples.append(augmented)\n",
    "                    all_labels.append(class_idx)\n",
    "                    class_counts[class_name] += 1\n",
    "                \n",
    "                # 2. قلب الإشارة لبعض الأعمدة\n",
    "                for flip_ratio in [0.2, 0.3, 0.4]:\n",
    "                    flipped = features.copy()\n",
    "                    flip_cols = np.random.choice(features.shape[1], size=int(features.shape[1] * flip_ratio), replace=False)\n",
    "                    flipped[:, flip_cols] = -flipped[:, flip_cols]\n",
    "                    all_samples.append(flipped)\n",
    "                    all_labels.append(class_idx)\n",
    "                    class_counts[class_name] += 1\n",
    "                \n",
    "                if features.shape[0] > 10:\n",
    "                    for shift in range(1, 6):\n",
    "                        shifted = np.roll(features, shift, axis=0)\n",
    "                        all_samples.append(shifted)\n",
    "                        all_labels.append(class_idx)\n",
    "                        class_counts[class_name] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {csv_file}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_frames = max([sample.shape[0] for sample in all_samples])\n",
    "max_features = max([sample.shape[1] for sample in all_samples])\n",
    "\n",
    "processed_samples = []\n",
    "for sample in all_samples:\n",
    "    if sample.shape[0] < max_frames:\n",
    "        pad = np.zeros((max_frames - sample.shape[0], sample.shape[1]))\n",
    "        sample = np.vstack((sample, pad))\n",
    "    elif sample.shape[0] > max_frames:\n",
    "        sample = sample[:max_frames, :]\n",
    "    \n",
    "    if sample.shape[1] < max_features:\n",
    "        pad = np.zeros((sample.shape[0], max_features - sample.shape[1]))\n",
    "        sample = np.hstack((sample, pad))\n",
    "    elif sample.shape[1] > max_features:\n",
    "        sample = sample[:, :max_features]\n",
    "    \n",
    "    processed_samples.append(sample)\n",
    "\n",
    "X = np.array(processed_samples)\n",
    "y = np.array(all_labels)\n",
    "\n",
    "# إعادة تشكيل البيانات لـ CNN-LSTM\n",
    "X = X.reshape(-1, 1, max_frames, max_features)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ImprovedCNNLSTM(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_size, num_classes, dropout_rate=0.5):\n",
    "        super(ImprovedCNNLSTM, self).__init__()\n",
    "        \n",
    "        self.time_steps, self.features = input_shape\n",
    "        \n",
    "        # First CNN block - detect basic features\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.dropout1 = nn.Dropout2d(0.3)\n",
    "        \n",
    "        # Second CNN block - detect motion patterns\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(5, 5), padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.dropout2 = nn.Dropout2d(0.3)\n",
    "        \n",
    "        # Third CNN block - detect high-level patterns\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(3, 3), padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.dropout3 = nn.Dropout2d(0.3)\n",
    "        \n",
    "        # Calculate CNN output size\n",
    "        self.cnn_output_size = self._get_conv_output_size(input_shape)\n",
    "        \n",
    "        # Bidirectional GRU\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.cnn_output_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        \n",
    "        # Enhanced attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Deep classifier\n",
    "        hidden_dims = [hidden_size * 2, 512, 256, 128]\n",
    "        \n",
    "        self.classifier = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            self.classifier.extend([\n",
    "                nn.Linear(hidden_dims[i], hidden_dims[i+1]),\n",
    "                nn.BatchNorm1d(hidden_dims[i+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate if i == 0 else dropout_rate / 2)\n",
    "            ])\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.final_fc = nn.Linear(hidden_dims[-1], num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d, nn.LayerNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _get_conv_output_size(self, input_shape):\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, 1, input_shape[0], input_shape[1])\n",
    "            x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "            x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "            x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "            return x.size(1) * x.size(2) * x.size(3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # CNN feature extraction\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # Reshape for RNN\n",
    "        x = x.view(batch_size, -1, self.cnn_output_size)\n",
    "        \n",
    "        # RNN with attention\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_weights = self.attention(rnn_out)\n",
    "        context = torch.sum(attn_weights * rnn_out, dim=1)\n",
    "        \n",
    "        # Deep classifier with skip connections\n",
    "        x = context\n",
    "        for i in range(0, len(self.classifier), 4):\n",
    "            identity = x\n",
    "            x = self.classifier[i](x)      # Linear\n",
    "            x = self.classifier[i+1](x)    # BatchNorm\n",
    "            x = self.classifier[i+2](x)    # ReLU\n",
    "            x = self.classifier[i+3](x)    # Dropout\n",
    "            \n",
    "            # Add skip connection if dimensions match\n",
    "            if identity.shape[1] == x.shape[1]:\n",
    "                x = x + identity\n",
    "        \n",
    "        # Final classification\n",
    "        x = self.final_fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImprovedCNNLSTM(\n",
    "    input_shape=(max_frames, max_features),\n",
    "    hidden_size=256,  \n",
    "    num_classes=len(CLASSES),\n",
    "    dropout_rate=0.5\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_counts = np.bincount(y_train)\n",
    "total_samples = len(y_train)\n",
    "\n",
    "max_count = max(class_counts)\n",
    "class_weights = torch.FloatTensor([max_count / (count + 1e-5) for count in class_counts])\n",
    "class_weights = class_weights / class_weights.sum() * len(CLASSES)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0005,  \n",
    "    weight_decay=0.01,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=True,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "batch_size = 64  \n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training improved model...\n",
      "Epoch [1/30] Train Acc: 21.50% | Val Acc: 17.42%\n",
      "Epoch [2/30] Train Acc: 32.58% | Val Acc: 48.39%\n",
      "Epoch [3/30] Train Acc: 43.73% | Val Acc: 53.55%\n",
      "Epoch [4/30] Train Acc: 48.91% | Val Acc: 58.39%\n",
      "Epoch [5/30] Train Acc: 52.71% | Val Acc: 67.74%\n",
      "Epoch [6/30] Train Acc: 54.49% | Val Acc: 68.06%\n",
      "Epoch [7/30] Train Acc: 54.97% | Val Acc: 69.03%\n",
      "Epoch [8/30] Train Acc: 64.35% | Val Acc: 76.13%\n",
      "Epoch [9/30] Train Acc: 63.62% | Val Acc: 73.87%\n",
      "Epoch [10/30] Train Acc: 64.83% | Val Acc: 78.39%\n",
      "Epoch [11/30] Train Acc: 66.94% | Val Acc: 72.90%\n",
      "Epoch [12/30] Train Acc: 69.44% | Val Acc: 78.39%\n",
      "Epoch [13/30] Train Acc: 69.77% | Val Acc: 75.48%\n",
      "Epoch [14/30] Train Acc: 70.49% | Val Acc: 81.61%\n",
      "Epoch [15/30] Train Acc: 71.71% | Val Acc: 82.26%\n",
      "Epoch [16/30] Train Acc: 70.98% | Val Acc: 84.84%\n",
      "Epoch [17/30] Train Acc: 72.43% | Val Acc: 78.06%\n",
      "Epoch [18/30] Train Acc: 72.11% | Val Acc: 86.45%\n",
      "Epoch [19/30] Train Acc: 74.05% | Val Acc: 82.90%\n",
      "Epoch [20/30] Train Acc: 74.62% | Val Acc: 86.13%\n",
      "Epoch [21/30] Train Acc: 77.77% | Val Acc: 89.03%\n",
      "Epoch [22/30] Train Acc: 76.15% | Val Acc: 84.19%\n",
      "Epoch [23/30] Train Acc: 77.69% | Val Acc: 87.42%\n",
      "Epoch [24/30] Train Acc: 77.45% | Val Acc: 86.77%\n",
      "Epoch [25/30] Train Acc: 79.87% | Val Acc: 87.10%\n",
      "Epoch [26/30] Train Acc: 81.41% | Val Acc: 90.32%\n",
      "Epoch [27/30] Train Acc: 79.55% | Val Acc: 85.81%\n",
      "Epoch [28/30] Train Acc: 78.66% | Val Acc: 88.71%\n",
      "Epoch [29/30] Train Acc: 81.08% | Val Acc: 90.32%\n",
      "Epoch [30/30] Train Acc: 81.08% | Val Acc: 90.65%\n"
     ]
    }
   ],
   "source": [
    "print(\"Training improved model...\")\n",
    "history = {\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "    'train_loss': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "\n",
    "best_val_accuracy = 0\n",
    "best_model_state = None\n",
    "patience = 15  \n",
    "counter = 0\n",
    "num_epochs = 30  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # تقييد أقوى\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += batch_y.size(0)\n",
    "            val_correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    val_loss /= len(test_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    \n",
    "    scheduler.step(val_accuracy)\n",
    "    \n",
    "    history['train_acc'].append(train_accuracy)\n",
    "    history['val_acc'].append(val_accuracy)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] Train Acc: {train_accuracy:.2f}% | Val Acc: {val_accuracy:.2f}%')\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy: 90.65%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"Best validation accuracy: {best_val_accuracy:.2f}%\")\n",
    "\n",
    "torch.save(model.state_dict(), 'ut_interaction_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_video_files():\n",
    "    video_files = []\n",
    "    video_dirs = [\n",
    "        \".\",\n",
    "        \"videos\",\n",
    "        \"data\",\n",
    "        os.path.join(\"drive\", \"data\"),\n",
    "        os.path.join(data_dir)\n",
    "    ]\n",
    "    \n",
    "    for video_dir in video_dirs:\n",
    "        if os.path.exists(video_dir):\n",
    "            for ext in ['.mp4', '.avi', '.mov', '.mkv']:\n",
    "                video_files.extend(glob.glob(os.path.join(video_dir, f\"*{ext}\")))\n",
    "    \n",
    "    for category in FOLDER_TO_CLASS.keys():\n",
    "        for video_dir in video_dirs:\n",
    "            category_dir = os.path.join(video_dir, category)\n",
    "            if os.path.exists(category_dir):\n",
    "                for ext in ['.mp4', '.avi', '.mov', '.mkv']:\n",
    "                    video_files.extend(glob.glob(os.path.join(category_dir, f\"*{ext}\")))\n",
    "    \n",
    "    return video_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, model, device):\n",
    "    print(f\"Processing video: {os.path.basename(video_path)}\")\n",
    "    \n",
    "    # Open video to get properties\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Could not open video: {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties first\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    original_frames = []\n",
    "    \n",
    "    # Read all frames first\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        original_frames.append(frame.copy())\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    if len(original_frames) == 0:\n",
    "        print(\"No frames could be extracted from the video\")\n",
    "        return\n",
    "    \n",
    "    # Try to find corresponding CSV file\n",
    "    video_basename = os.path.basename(video_path)\n",
    "    video_name = video_basename.split('.')[0]\n",
    "    video_dir = os.path.dirname(video_path)\n",
    "    \n",
    "    # List of possible CSV locations\n",
    "    csv_locations = [\n",
    "        os.path.join(video_dir, f\"{video_name}.csv\"),\n",
    "        os.path.join(\"csv\", f\"{video_name}.csv\"),\n",
    "        os.path.join(\"drive/csv\", f\"{video_name}.csv\"),\n",
    "        os.path.join(data_dir, \"csv\", f\"{video_name}.csv\")\n",
    "    ]\n",
    "    \n",
    "    # Try to find CSV file\n",
    "    csv_file = None\n",
    "    for loc in csv_locations:\n",
    "        if os.path.exists(loc):\n",
    "            csv_file = loc\n",
    "            break\n",
    "    \n",
    "    features = None\n",
    "    if csv_file:\n",
    "        try:\n",
    "            # Load CSV data\n",
    "            df = pd.read_csv(csv_file)\n",
    "            if 'Unnamed: 0' in df.columns:\n",
    "                df = df.drop('Unnamed: 0', axis=1)\n",
    "            \n",
    "            features = df.values\n",
    "            if features.size > 0:\n",
    "                features = (features - features.mean()) / (features.std() + 1e-5)\n",
    "                print(f\"Successfully loaded CSV data from: {csv_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading CSV file: {str(e)}\")\n",
    "            features = None\n",
    "    \n",
    "    # If no CSV data, extract features from video frames\n",
    "    if features is None:\n",
    "        print(\"No CSV data found, extracting features from video frames...\")\n",
    "        frames = []\n",
    "        \n",
    "        for i, frame in enumerate(original_frames):\n",
    "            # Enhanced feature extraction\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            resized = cv2.resize(gray, (64, 64))\n",
    "            \n",
    "            # Extract multiple features\n",
    "            edges = cv2.Canny(resized, 100, 200)\n",
    "            \n",
    "            if i > 0:\n",
    "                prev_frame = cv2.resize(cv2.cvtColor(original_frames[i-1], cv2.COLOR_BGR2GRAY), (64, 64))\n",
    "                flow = cv2.calcOpticalFlowFarneback(prev_frame, resized, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "                \n",
    "                combined_features = np.concatenate([\n",
    "                    edges.flatten(),\n",
    "                    magnitude.flatten(),\n",
    "                    angle.flatten()\n",
    "                ])\n",
    "            else:\n",
    "                combined_features = np.concatenate([\n",
    "                    edges.flatten(),\n",
    "                    np.zeros(64*64),\n",
    "                    np.zeros(64*64)\n",
    "                ])\n",
    "            \n",
    "            frames.append(combined_features)\n",
    "        \n",
    "        features = np.array(frames)\n",
    "        features = (features - features.mean()) / (features.std() + 1e-5)\n",
    "    \n",
    "    # Process features for model input\n",
    "    if features.shape[0] < max_frames:\n",
    "        pad = np.zeros((max_frames - features.shape[0], features.shape[1]))\n",
    "        features = np.vstack((features, pad))\n",
    "    else:\n",
    "        features = features[:max_frames, :]\n",
    "    \n",
    "    if features.shape[1] < max_features:\n",
    "        pad = np.zeros((features.shape[0], max_features - features.shape[1]))\n",
    "        features = np.hstack((features, pad))\n",
    "    else:\n",
    "        features = features[:, :max_features]\n",
    "    \n",
    "    # Reshape for model\n",
    "    features = features.reshape(1, 1, max_frames, max_features)\n",
    "    features_tensor = torch.FloatTensor(features).to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(features_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)[0]\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        pred_class_idx = predicted.item()\n",
    "    \n",
    "    pred_class = CLASSES[pred_class_idx]\n",
    "    confidence = probabilities[pred_class_idx].item() * 100\n",
    "    \n",
    "    print(f\"\\nPrediction Results:\")\n",
    "    print(f\"Class: {pred_class}\")\n",
    "    print(f\"Description: {ACTION_DESCRIPTIONS[pred_class]}\")\n",
    "    print(f\"Confidence: {confidence:.2f}%\")\n",
    "    print(\"\\nTop 3 Predictions:\")\n",
    "    \n",
    "    # Get top 3 predictions\n",
    "    top3_values, top3_indices = torch.topk(probabilities, 3)\n",
    "    for i, idx in enumerate(top3_indices):\n",
    "        cls = CLASSES[idx]\n",
    "        prob = top3_values[i].item() * 100\n",
    "        print(f\"{cls}: {prob:.2f}% - {ACTION_DESCRIPTIONS[cls]}\")\n",
    "    \n",
    "    # Create output video with predictions\n",
    "    output_filename = f\"output_{video_basename.split('.')[0]}.mp4\"\n",
    "    out = cv2.VideoWriter(output_filename, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "    \n",
    "    # Add predictions to video frames\n",
    "    for frame in original_frames:\n",
    "        # Add black background for text\n",
    "        cv2.rectangle(frame, (0, 0), (height//4, 80), (0, 0, 0), -1)\n",
    "        \n",
    "        # Add prediction and confidence\n",
    "        cv2.putText(frame, f\"Prediction: {pred_class} ({confidence:.1f}%)\", \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        \n",
    "        # Add description\n",
    "        description = ACTION_DESCRIPTIONS[pred_class]\n",
    "        cv2.putText(frame, description, \n",
    "                   (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        \n",
    "        out.write(frame)\n",
    "    \n",
    "    out.release()\n",
    "    print(f\"\\nOutput saved as: {output_filename}\")\n",
    "    return pred_class, confidence, dict(zip(CLASSES, probabilities.cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 240 video files\n",
      "1. 10_2_1.avi\n",
      "2. 14_3_1.avi\n",
      "3. 19_14_1.avi\n",
      "4. 19_4_1.avi\n",
      "5. 24_15_1.avi\n",
      "Processing video: 10_2_1.avi\n",
      "No CSV data found, extracting features from video frames...\n",
      "\n",
      "Prediction Results:\n",
      "Class: PUSHING\n",
      "Description: Person using force to push another person\n",
      "Confidence: 57.34%\n",
      "\n",
      "Top 3 Predictions:\n",
      "PUSHING: 57.34% - Person using force to push another person\n",
      "HUGGING: 33.82% - Two people embracing each other in a friendly manner\n",
      "HANDSHAKING: 8.25% - Two people engaging in a formal handshake greeting\n",
      "\n",
      "Output saved as: output_10_2_1.mp4\n",
      "\n",
      "Generating final evaluation plots...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     HUGGING       1.00      1.00      1.00        50\n",
      "     KICKING       0.75      0.87      0.80        52\n",
      "    PUNCHING       0.82      0.71      0.76        52\n",
      "     PUSHING       0.91      0.98      0.94        52\n",
      " HANDSHAKING       0.98      0.89      0.93        55\n",
      "    POINTING       1.00      1.00      1.00        49\n",
      "\n",
      "    accuracy                           0.91       310\n",
      "   macro avg       0.91      0.91      0.91       310\n",
      "weighted avg       0.91      0.91      0.91       310\n",
      "\n",
      "\n",
      "Plotting class distribution...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model on video\n",
    "video_files = find_video_files()\n",
    "if video_files:\n",
    "    print(f\"Found {len(video_files)} video files\")\n",
    "    for i, video in enumerate(video_files[:5]):\n",
    "        print(f\"{i+1}. {os.path.basename(video)}\")\n",
    "    \n",
    "    # Process first video\n",
    "    video_path = video_files[0]\n",
    "    process_video(video_path, model, device)\n",
    "else:\n",
    "    print(\"No video files found. Please place video files in one of these directories:\")\n",
    "    print(\"- Current directory (.)\")\n",
    "    print(\"- videos/\")\n",
    "    print(\"- data/\")\n",
    "    print(\"- drive/data/\")\n",
    "    print(\"- Or in category subdirectories (e.g., videos/hugging/, data/kicking/, etc.)\")\n",
    "\n",
    "# Add after the training loop ends, add:\n",
    "print(\"\\nGenerating final evaluation plots...\")\n",
    "plot_training_history(history)\n",
    "\n",
    "# Evaluate model and generate confusion matrix\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=CLASSES))\n",
    "\n",
    "# Add after data splitting\n",
    "print(\"\\nPlotting class distribution...\")\n",
    "plot_class_distribution(y_train, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 241 video files\n",
      "1. output_10_2_1.mp4\n",
      "2. 10_2_1.avi\n",
      "3. 14_3_1.avi\n",
      "4. 19_14_1.avi\n",
      "5. 19_4_1.avi\n",
      "Processing video: output_10_2_1.mp4\n",
      "No CSV data found, extracting features from video frames...\n",
      "\n",
      "Prediction Results:\n",
      "Class: PUSHING\n",
      "Description: Person using force to push another person\n",
      "Confidence: 98.81%\n",
      "\n",
      "Top 3 Predictions:\n",
      "PUSHING: 98.81% - Person using force to push another person\n",
      "PUNCHING: 0.65% - Person throwing a punch or striking motion\n",
      "HANDSHAKING: 0.38% - Two people engaging in a formal handshake greeting\n",
      "\n",
      "Output saved as: output_output_10_2_1.mp4\n",
      "\n",
      "Generating final evaluation plots...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     HUGGING       1.00      1.00      1.00        50\n",
      "     KICKING       0.75      0.87      0.80        52\n",
      "    PUNCHING       0.82      0.71      0.76        52\n",
      "     PUSHING       0.91      0.98      0.94        52\n",
      " HANDSHAKING       0.98      0.89      0.93        55\n",
      "    POINTING       1.00      1.00      1.00        49\n",
      "\n",
      "    accuracy                           0.91       310\n",
      "   macro avg       0.91      0.91      0.91       310\n",
      "weighted avg       0.91      0.91      0.91       310\n",
      "\n",
      "\n",
      "Plotting class distribution...\n"
     ]
    }
   ],
   "source": [
    "video_files = find_video_files()\n",
    "if video_files:\n",
    "    print(f\"Found {len(video_files)} video files\")\n",
    "    for i, video in enumerate(video_files[:5]):\n",
    "        print(f\"{i+1}. {os.path.basename(video)}\")\n",
    "    \n",
    "    # Process first video\n",
    "    video_path = video_files[0]\n",
    "    process_video(video_path, model, device)\n",
    "else:\n",
    "    print(\"No video files found. Please place video files in one of these directories:\")\n",
    "    print(\"- Current directory (.)\")\n",
    "    print(\"- videos/\")\n",
    "    print(\"- data/\")\n",
    "    print(\"- drive/data/\")\n",
    "    print(\"- Or in category subdirectories (e.g., videos/hugging/, data/kicking/, etc.)\")\n",
    "\n",
    "# Add after the training loop ends, add:\n",
    "print(\"\\nGenerating final evaluation plots...\")\n",
    "plot_training_history(history)\n",
    "\n",
    "# Evaluate model and generate confusion matrix\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=CLASSES))\n",
    "\n",
    "# Add after data splitting\n",
    "print(\"\\nPlotting class distribution...\")\n",
    "plot_class_distribution(y_train, y_test) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
